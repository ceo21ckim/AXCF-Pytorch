{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from settings import * \n",
    "\n",
    "import torch \n",
    "import pandas as pd \n",
    "import dgl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(YELP_DIR, 'train.csv')\n",
    "valid_path = os.path.join(YELP_DIR, 'valid.csv')\n",
    "test_path = os.path.join(YELP_DIR, 'test.csv')\n",
    "\n",
    "d_train = pd.read_csv(train_path, encoding='utf-8-sig')\n",
    "d_valid = pd.read_csv(valid_path, encoding='utf-8-sig')\n",
    "d_test = pd.read_csv(test_path, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uids = torch.LongTensor(d_train.loc[:, 'user_id'])\n",
    "train_iids = torch.LongTensor(d_train.loc[:, 'business_id'])\n",
    "train_ratings = torch.LongTensor(d_train.loc[:, 'stars'])\n",
    "\n",
    "valid_uids = torch.LongTensor(d_valid.loc[:, 'user_id'])\n",
    "valid_iids = torch.LongTensor(d_valid.loc[:, 'business_id'])\n",
    "valid_ratings = torch.LongTensor(d_valid.loc[:, 'stars'])\n",
    "\n",
    "test_uids = torch.LongTensor(d_test.loc[:, 'user_id'])\n",
    "test_iids = torch.LongTensor(d_test.loc[:, 'business_id'])\n",
    "test_ratings = torch.LongTensor(d_test.loc[:, 'stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dgl.heterograph({\n",
    "    ('user', 'preference', 'item'): (train_uids, train_iids), \n",
    "    ('item', 'preference-by', 'user') : (train_iids, train_uids)\n",
    "})\n",
    "\n",
    "graph.edges['preference'].data['rating'] = train_ratings \n",
    "graph.edges['preference-by'].data['rating'] = train_ratings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "\n",
    "train_set = TensorDataset(train_uids, train_iids, train_ratings)\n",
    "valid_set = TensorDataset(valid_uids, valid_iids, valid_ratings)\n",
    "test_set = TensorDataset(test_uids, test_iids, test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchSampler:\n",
    "    def __init__(self, graph, num_layers):\n",
    "        self.graph = graph \n",
    "        self.num_layers = num_layers \n",
    "\n",
    "    def sample(self, batch):\n",
    "        users, items, ratings = zip(*batch)\n",
    "        users = torch.stack(users)\n",
    "        items = torch.stack(items)\n",
    "        ratings = torch.stack(ratings)\n",
    "\n",
    "        pair_graph = dgl.heterograph(\n",
    "            {('user', 'preference', 'item') : (users, items)}, \n",
    "            num_nodes_dict = {'user' : self.graph.number_of_nodes('user'), 'item': self.graph.number_of_nodes('item')}\n",
    "        )\n",
    "\n",
    "        pair_graph = dgl.compact_graphs(pair_graph)\n",
    "\n",
    "        pair_graph.edata['rating'] = ratings\n",
    "\n",
    "        seeds = {'user': pair_graph.nodes['user'].data[dgl.NID], \n",
    "                 'item': pair_graph.nodes['item'].data[dgl.NID]}\n",
    "        blocks = self.construct_blocks(seeds, (users, items))\n",
    "\n",
    "        for feature_name in self.graph.nodes['user'].data.keys():\n",
    "            blocks[0].srcnodes['user'].data[feature_name] = self.graph.nodes['user'].data[feature_name][blocks[0].sronodes['user'].data[dgl.NID]]\n",
    "\n",
    "        for feature_name in self.graph.nodes['item'].data.keys():\n",
    "            blocks[0].srcnodes['item'].data[feature_name] = self.graph.nodes['item'].data[feature_name][blocks[0].srcnodes['item'].data[dgl.NID]]\n",
    "\n",
    "        return pair_graph, blocks\n",
    "\n",
    "    def construct_blocks(self, seeds, user_item_pairs_to_remove):\n",
    "        blocks = []\n",
    "        users, items = user_item_pairs_to_remove\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            sampled_graph = dgl.in_subgraph(self.graph, seeds)\n",
    "\n",
    "            sampled_eids = sampled_graph.edges['preference'].data[dgl.EID]\n",
    "            sampled_eids_rev = sampled_graph.edges['preference-by'].data[dgl.EID]\n",
    "            \n",
    "            # rating을 예측하는 것은 edge를 예측하는 것과 같으며, \n",
    "            # sub graph의 edge를 예측할 때 모델이 연결되어 있다는 정보를 알지 못하도록 remove 합니다.\n",
    "            # 모델이 연결되어 있다는 정보를 알고 있다면, 예측의 의미가 없기 때문입니다.\n",
    "            _, _, edges_to_remove = sampled_graph.edge_ids(users, items, etype='preference', return_uv=True)\n",
    "            _, _, edges_to_remove_rev = sampled_graph.edge_ids(items, users, etype='preference-by', return_uv=True)\n",
    "            \n",
    "            sampled_with_edges_removed = sampled_graph\n",
    "            if len(edges_to_remove) > 0:\n",
    "                sampled_with_edges_removed = dgl.remove_edges(\n",
    "                    sampled_with_edges_removed, edges_to_remove, 'preference')\n",
    "                sampled_eids = sampled_with_edges_removed.edges['preference'].data[dgl.EID]\n",
    "            if len(edges_to_remove_rev) > 0:\n",
    "                sampled_with_edges_removed = dgl.remove_edges(\n",
    "                    sampled_with_edges_removed, edges_to_remove_rev, 'preference-by')\n",
    "                sampled_eids_rev = sampled_with_edges_removed.edges['preference-by'].data[dgl.EID]\n",
    "            \n",
    "            # Create a block from the sampled graph.\n",
    "            block = dgl.to_block(sampled_with_edges_removed, seeds)\n",
    "            blocks.insert(0, block)\n",
    "            seeds = {'user': block.srcnodes['user'].data[dgl.NID],\n",
    "                     'item': block.srcnodes['item'].data[dgl.NID]}\n",
    "            \n",
    "            # Copy the ratings to the edges of the sampled block\n",
    "            block.edges['preference'].data['rating'] = \\\n",
    "                self.graph.edges['preference'].data['rating'][sampled_eids]\n",
    "            block.edges['preference-by'].data['rating'] = \\\n",
    "                self.graph.edges['preference-by'].data['rating'][sampled_eids_rev]\n",
    "            \n",
    "        return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import dgl.function as fn \n",
    "import dgl.nn as dglnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCMCConv(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_ratings):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The ratings are ranged from 1 to num_ratings, so I add 1 to the number of parameters.\n",
    "        self.W_r = nn.Parameter(torch.randn(num_ratings + 1, hidden_dims, hidden_dims))\n",
    "        self.W = nn.Linear(hidden_dims * 2, hidden_dims)\n",
    "        \n",
    "    def compute_message(self, W, edges):\n",
    "        W_r = W[edges.data['rating']]\n",
    "        h = edges.src['h']\n",
    "        m = (W_r @ h.unsqueeze(-1)).squeeze(2)\n",
    "        return m\n",
    "    \n",
    "    def forward(self, graph, node_features):\n",
    "        with graph.local_scope():\n",
    "            src_features, dst_features = node_features\n",
    "            graph.srcdata['h'] = src_features\n",
    "            graph.dstdata['h'] = dst_features\n",
    "            # Compute messages\n",
    "            graph.apply_edges(lambda edges: {'m': self.compute_message(self.W_r, edges)})\n",
    "            # Aggregate messages\n",
    "            graph.update_all(fn.copy_e('m', 'm'), fn.mean('m', 'h_neigh'))\n",
    "            # Updates the representations of output users and items\n",
    "            result = F.relu(self.W(torch.cat([graph.dstdata['h'], graph.dstdata['h_neigh']], 1)))\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCMCLayer(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_ratings):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heteroconv = dglnn.HeteroGraphConv(\n",
    "            {'preference': GCMCConv(hidden_dims, num_ratings), 'preference-by': GCMCConv(hidden_dims, num_ratings)}, \n",
    "            aggregate='sum'\n",
    "        )\n",
    "    \n",
    "    def forward(self, block, input_user_features, input_item_features):\n",
    "        with block.local_scope():\n",
    "            h_user = input_user_features \n",
    "            h_item = input_item_features \n",
    "            \n",
    "            src_features = {'user':h_user, 'item':h_item}\n",
    "            \n",
    "            dst_features = {'user':h_user[:block.number_of_dst_nodes('user')], 'item': h_item[:block.number_of_dst_nodes('item')]}\n",
    "            \n",
    "            result = self.heteroconv(block, (src_features, dst_features))\n",
    "            return result['user'], result['item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCMCRating(nn.Module):\n",
    "    def __init__(self, num_users, num_items, hidden_dims, num_ratings, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Node-specific learnable embeddings\n",
    "        self.user_embeddings = nn.Embedding(num_users, hidden_dims)\n",
    "        self.item_embeddings = nn.Embedding(num_items, hidden_dims)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            GCMCLayer(hidden_dims, num_ratings) for _ in range(num_layers)])\n",
    "        \n",
    "        self.W = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.V = nn.Linear(hidden_dims, hidden_dims)\n",
    "        \n",
    "    def forward(self, blocks):\n",
    "        # Propagate messages top-down (Step 4)\n",
    "        # We start with a learnable embedding for each user and item...\n",
    "        user_embeddings = self.user_embeddings(blocks[0].srcnodes['user'].data[dgl.NID])\n",
    "        item_embeddings = self.item_embeddings(blocks[0].srcnodes['item'].data[dgl.NID])\n",
    "        \n",
    "        \n",
    "        # Then perform a heterogeneous GCMC convolution\n",
    "        for block, layer in zip(blocks, self.layers):\n",
    "            user_embeddings, item_embeddings = layer(block, user_embeddings, item_embeddings)\n",
    "        \n",
    "        # Compute predicted preference (Step 5)\n",
    "        user_embeddings = self.W(user_embeddings)\n",
    "        item_embeddings = self.V(item_embeddings)\n",
    "        \n",
    "        return user_embeddings, item_embeddings\n",
    "        \n",
    "    def compute_score(self, pair_graph, user_embeddings, item_embeddings):\n",
    "        with pair_graph.local_scope():\n",
    "            pair_graph.nodes['user'].data['h'] = user_embeddings\n",
    "            pair_graph.nodes['item'].data['h'] = item_embeddings\n",
    "            pair_graph.apply_edges(fn.u_dot_v('h', 'h', 'r'))\n",
    "            \n",
    "            return pair_graph.edata['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred, label):\n",
    "    pred = pred.flatten()\n",
    "    return ((pred - label)**2).mean().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1448/1448 [01:04<00:00, 22.35it/s, loss=1.4018]\n",
      "100%|██████████| 480/480 [00:07<00:00, 60.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 164/1448 [00:07<00:58, 22.03it/s, loss=1.3607]"
     ]
    }
   ],
   "source": [
    "import tqdm \n",
    "\n",
    "NUM_LAYERS = 1 \n",
    "BATCH_SIZE = 500 \n",
    "NUM_EPOCHS = 50 \n",
    "HIDDEN_DIMS = 8\n",
    "\n",
    "sampler = MinibatchSampler(graph, NUM_LAYERS)\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=sampler.sample, shuffle=True)\n",
    "test_dataloader = DataLoader(valid_set, batch_size=BATCH_SIZE, collate_fn=sampler.sample, shuffle=False)\n",
    "\n",
    "model = GCMCRating(graph.number_of_nodes('user'), graph.number_of_nodes('item'), HIDDEN_DIMS, 5, NUM_LAYERS)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "best_loss = float('inf')\n",
    "for _ in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(train_dataloader) as t:\n",
    "        for pair_graph, blocks in t: \n",
    "           \n",
    "            user_emb, item_emb = model(blocks)\n",
    "            prediction = model.compute_score(pair_graph, user_emb, item_emb)\n",
    "            loss = ((prediction - pair_graph.edata['rating']) ** 2).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            t.set_postfix({'loss': '%.4f' % loss.item()}, refresh=False)\n",
    "    \n",
    "    model.eval()\n",
    "    with tqdm.tqdm(test_dataloader) as t:\n",
    "        with torch.no_grad():\n",
    "            predictions = [] \n",
    "            ratings = []\n",
    "            for pair_graph, blocks in t:\n",
    "                user_emb, item_emb = model(blocks)\n",
    "                prediction = model.compute_score(pair_graph, user_emb, item_emb)\n",
    "                predictions.append(prediction)\n",
    "                ratings.append(pair_graph.edata['rating'])\n",
    "            \n",
    "            predictions = torch.cat(predictions, dim=0)\n",
    "            ratings = torch.cat(ratings, dim=0)\n",
    "        RMSE_loss = rmse(predictions, ratings)\n",
    "        print(f'RMSE: {RMSE_loss.item():.4f}', )\n",
    "\n",
    "        if best_loss > RMSE_loss:\n",
    "            best_loss = RMSE_loss \n",
    "            path = os.path.join(SAVE_PATH)\n",
    "            torch.save(model.state_dict(), os.path.join(path, 'GCMC-paramters.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
