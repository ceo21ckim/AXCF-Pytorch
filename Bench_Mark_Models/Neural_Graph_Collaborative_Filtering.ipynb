{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import os \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import scipy.sparse  as sp \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "import torch \n",
    "from torch import nn, optim \n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    seed = 42\n",
    "    num_layers = 4\n",
    "    batch_size= 4096\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    SAVE_PATH = 'Parameters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_set = pd.read_csv('dataset/Yelp2018/Yelp2018.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train, d_test = train_test_split(d_set, train_size=0.6, random_state=args.seed)\n",
    "d_valid, d_test = train_test_split(d_test, train_size=0.5, random_state=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = d_train.astype({'user_id':'category', 'business_id':'category'})\n",
    "d_valid = d_valid.astype({'user_id':'category', 'business_id':'category'})\n",
    "d_test = d_test.astype({'user_id':'category', 'business_id':'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_cat = d_train.user_id.cat.categories\n",
    "b_cat = d_train.business_id.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_valid.user_id = d_valid.user_id.cat.set_categories(u_cat)\n",
    "d_valid.business_id = d_valid.business_id.cat.set_categories(b_cat)\n",
    "\n",
    "d_test.user_id = d_test.user_id.cat.set_categories(u_cat)\n",
    "d_test.business_id = d_test.business_id.cat.set_categories(b_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train.user_id = d_train.user_id.cat.codes\n",
    "d_train.business_id = d_train.business_id.cat.codes \n",
    "\n",
    "d_valid.user_id = d_valid.user_id.cat.codes\n",
    "d_valid.business_id = d_valid.business_id.cat.codes \n",
    "\n",
    "d_test.user_id = d_test.user_id.cat.codes\n",
    "d_test.business_id = d_test.business_id.cat.codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = d_train.dropna()\n",
    "d_valid = d_valid.dropna()\n",
    "d_test = d_test.dropna()\n",
    "\n",
    "d_train.reset_index(drop=True, inplace=True)\n",
    "d_valid.reset_index(drop=True, inplace=True)\n",
    "d_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = d_train.astype({'user_id': int, 'business_id': int})\n",
    "d_valid = d_valid.astype({'user_id': int, 'business_id': int})\n",
    "d_test = d_test.astype({'user_id': int, 'business_id': int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_users = d_train.user_id.max() + 1\n",
    "args.num_items = d_train.business_id.max() + 1\n",
    "args.latent_dim = 64\n",
    "args.num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.out_feats = out_feats \n",
    "\n",
    "        self.W1 = nn.Linear(in_feats, out_feats)\n",
    "        self.W2 = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, L, SelfLoop, feats):\n",
    "        # (L+I)EW_1\n",
    "        sf_L = L + SelfLoop\n",
    "        L = L.cuda()\n",
    "        sf_L = sf_L.cuda()\n",
    "        sf_E = torch.sparse.mm(sf_L, feats)\n",
    "        left_part = self.W1(sf_E) # left part\n",
    "\n",
    "        # EL odot EW_2, odot indicates element-wise product \n",
    "        LE = torch.sparse.mm(L, feats)\n",
    "        E = torch.mul(LE, feats)\n",
    "        right_part = self.W2(E)\n",
    "\n",
    "        return left_part + right_part \n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "# class GNNLayer(nn.Module):\n",
    "#     def __init__(self, in_feats, out_feats):\n",
    "#         super(GNNLayer, self).__init__()\n",
    "#         self.in_feats = in_feats \n",
    "#         self.out_feats = out_feats \n",
    "#         self.linear1 = nn.Linear(self.in_feats, self.out_feats)\n",
    "#         self.linear2 = nn.Linear(self.in_feats, self.out_feats)\n",
    "        \n",
    "#     def forward(self, L, selfLoop, features):\n",
    "#         L1 = L + selfLoop\n",
    "#         L2 = L.cuda()\n",
    "#         L1 = L1.cuda()\n",
    "#         inter_feats = torch.sparse.mm(L2, features)\n",
    "#         inter_feats = torch.mul(inter_feats, features)\n",
    "        \n",
    "#         inter_part1 = self.linear1(torch.sparse.mm(L1, features))\n",
    "#         inter_part2 = self.linear2(torch.sparse.mm(L2, inter_feats))\n",
    "        \n",
    "#         return inter_part1 + inter_part2 \n",
    "\n",
    "class NGCF(nn.Module):\n",
    "    def __init__(self, args, matrix):\n",
    "        super(NGCF, self).__init__()\n",
    "        self.num_users = args.num_users \n",
    "        self.num_items = args.num_items \n",
    "        self.latent_dim = args.latent_dim \n",
    "        self.device = args.device\n",
    "\n",
    "        self.user_emb = nn.Embedding(self.num_users, self.latent_dim)\n",
    "        self.item_emb = nn.Embedding(self.num_items, self.latent_dim)\n",
    "\n",
    "        self.num_layers = args.num_layers\n",
    "        self.L = self.LaplacianMatrix(matrix)\n",
    "        self.I = self.SelfLoop(self.num_users + self.num_items)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.GNNLayers = nn.ModuleList()\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.GNNLayers.append(GNNLayer(self.latent_dim, self.latent_dim))\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim * self.num_layers * 2, 64), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, 32), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            \n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "\n",
    "    def SelfLoop(self, num):\n",
    "        i = torch.LongTensor([[k for k in range(0, num)], [j for j in range(0, num)]])\n",
    "        val = torch.FloatTensor([1]*num)\n",
    "        return torch.sparse.FloatTensor(i, val)\n",
    "\n",
    "    def LaplacianMatrix(self, ratings):\n",
    "        iids = ratings['business_id'] + self.num_users \n",
    "        matrix = sp.coo_matrix((ratings['stars'], (ratings['user_id'], ratings['business_id'])))\n",
    "        \n",
    "        upper_matrix = sp.coo_matrix((ratings['stars'], (ratings['user_id'], iids)))\n",
    "        lower_matrix = matrix.transpose()\n",
    "        lower_matrix.resize((self.num_items, self.num_users + self.num_items))\n",
    "\n",
    "        A = sp.vstack([upper_matrix, lower_matrix])\n",
    "        row_sum = (A > 0).sum(axis=1)\n",
    "        row_sum = np.array(row_sum).flatten()\n",
    "        # diag = list(np.array(row_sum.flatten())[0])\n",
    "        D = np.power(row_sum, -0.5)\n",
    "        D = sp.diags(D)\n",
    "        L = D * A * D\n",
    "        L = sp.coo_matrix(L)\n",
    "        row = L.row \n",
    "        col = L.col\n",
    "        idx = np.stack([row, col])\n",
    "        idx = torch.LongTensor(idx)\n",
    "        data = torch.FloatTensor(L.data)\n",
    "        SparseL = torch.sparse.FloatTensor(idx, data)\n",
    "        return SparseL \n",
    "\n",
    "    def FeatureMatrix(self):\n",
    "        uids = torch.LongTensor([i for i in range(self.num_users)]).to(self.device)\n",
    "        iids = torch.LongTensor([i for i in range(self.num_items)]).to(self.device)\n",
    "        user_emb = self.user_emb(uids)\n",
    "        item_emb = self.item_emb(iids)\n",
    "        features = torch.cat([user_emb, item_emb], dim=0)\n",
    "        return features\n",
    "\n",
    "    def forward(self, uids, iids):\n",
    "        iids = self.num_users + iids \n",
    "\n",
    "        features = self.FeatureMatrix()\n",
    "        final_emb = features.clone()\n",
    "\n",
    "        for gnn in self.GNNLayers:\n",
    "            features = gnn(self.L, self.I, features)\n",
    "            features = self.leakyrelu(features)\n",
    "            final_emb = torch.concat([final_emb, features],dim=-1)\n",
    "\n",
    "        user_emb = final_emb[uids]\n",
    "        item_emb = final_emb[iids]\n",
    "\n",
    "        inputs = torch.concat([user_emb, item_emb], dim=-1)\n",
    "        outs = self.fc_layer(inputs)\n",
    "        # outs = self.sigmoid(outs)\n",
    "        return outs.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        super(Dataset, self).__init__()\n",
    "        \n",
    "        self.uid = list(dataframe['user_id'])\n",
    "        self.iid = list(dataframe['business_id'])\n",
    "        self.ratings = list(dataframe['stars'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.uid)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        uid = self.uid[idx]\n",
    "        iid = self.iid[idx]\n",
    "        rating = self.ratings[idx]\n",
    "        \n",
    "        return (uid, iid, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(args, dataset, num_workers):\n",
    "    d_set = GraphDataset(dataset)\n",
    "    return DataLoader(d_set, batch_size=args.batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_loader(args, d_train, 4)\n",
    "valid_loader = get_loader(args, d_valid, 4)\n",
    "test_loader = get_loader(args, d_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_evaluate(args, model, test_loader, criterion):\n",
    "    output = []\n",
    "    test_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='evaluating...'):\n",
    "            batch = tuple(b.to(args.device) for b in batch)\n",
    "            inputs = {'uids':   batch[0], \n",
    "                      'iids':   batch[1]}\n",
    "            gold_y = batch[2].float()\n",
    "            \n",
    "            pred_y = model(**inputs)\n",
    "            output.append(pred_y)\n",
    "            \n",
    "            loss = criterion(pred_y, gold_y)\n",
    "            loss = torch.sqrt(loss)\n",
    "            test_loss += loss.item()\n",
    "    test_loss /= len(test_loader)\n",
    "    return test_loss, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_train(args, model, train_loader, valid_loader, optimizer, criterion):\n",
    "    best_loss = float('inf')\n",
    "    train_losses, valid_losses = [], []\n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader, desc='training...'):\n",
    "            batch = tuple(b.to(args.device) for b in batch)\n",
    "            inputs = {'uids':   batch[0], \n",
    "                      'iids':   batch[1]}\n",
    "            \n",
    "            gold_y = batch[2].float()\n",
    "            \n",
    "\n",
    "            pred_y = model(**inputs)\n",
    "            \n",
    "            loss = criterion(pred_y, gold_y)\n",
    "            loss = torch.sqrt(loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        valid_loss , outputs = graph_evaluate(args, model, valid_loader, criterion)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "\n",
    "        print(f'Epoch: [{epoch}/{args.num_epochs}]')\n",
    "        print(f'Train Loss: {train_loss:.4f}\\tValid Loss: {valid_loss:.4f}')\n",
    "\n",
    "        if best_loss > valid_loss:\n",
    "            best_loss = valid_loss\n",
    "            if not os.path.exists(args.SAVE_PATH):\n",
    "                os.makedirs(args.SAVE_PATH)\n",
    "            torch.save(model.state_dict(), os.path.join(args.SAVE_PATH, f'{model._get_name()}_parameters.pt'))\n",
    "\n",
    "    return {\n",
    "        'train_loss': train_losses, \n",
    "        'valid_loss': valid_losses\n",
    "    }, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = NGCF(args, d_train).to(args.device)\n",
    "\n",
    "optimizer = optim.Adam(models.parameters(), lr = 1e-4)\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:38<00:00,  4.56it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/15]\n",
      "Train Loss: 1.4006\tValid Loss: 0.9906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:39<00:00,  4.49it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/15]\n",
      "Train Loss: 0.9325\tValid Loss: 0.9288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:38<00:00,  4.55it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/15]\n",
      "Train Loss: 0.9168\tValid Loss: 0.9246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:37<00:00,  4.67it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4/15]\n",
      "Train Loss: 0.9139\tValid Loss: 0.9227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:38<00:00,  4.65it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/15]\n",
      "Train Loss: 0.9115\tValid Loss: 0.9209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:38<00:00,  4.57it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/15]\n",
      "Train Loss: 0.9088\tValid Loss: 0.9192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:38<00:00,  4.56it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/15]\n",
      "Train Loss: 0.9056\tValid Loss: 0.9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:38<00:00,  4.58it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8/15]\n",
      "Train Loss: 0.9020\tValid Loss: 0.9152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:39<00:00,  4.53it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/15]\n",
      "Train Loss: 0.8983\tValid Loss: 0.9134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:39<00:00,  4.49it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/15]\n",
      "Train Loss: 0.8946\tValid Loss: 0.9117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:39<00:00,  4.45it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11/15]\n",
      "Train Loss: 0.8910\tValid Loss: 0.9105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:39<00:00,  4.47it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12/15]\n",
      "Train Loss: 0.8878\tValid Loss: 0.9092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:39<00:00,  4.51it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13/15]\n",
      "Train Loss: 0.8848\tValid Loss: 0.9082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:39<00:00,  4.52it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:09<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14/15]\n",
      "Train Loss: 0.8820\tValid Loss: 0.9072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 177/177 [00:41<00:00,  4.23it/s]\n",
      "evaluating...: 100%|██████████| 59/59 [00:10<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15/15]\n",
      "Train Loss: 0.8794\tValid Loss: 0.9062\n"
     ]
    }
   ],
   "source": [
    "results = graph_train(args, models, train_loader, valid_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 59/59 [00:10<00:00,  5.73it/s]\n"
     ]
    }
   ],
   "source": [
    "inference_results = graph_evaluate(args, models, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs, yhat = inference_results\n",
    "yhat = torch.concat(yhat, dim=0).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test.loc[:, 'yhat'] = yhat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(x):\n",
    "    if x >= 3.5 : return 1\n",
    "    elif x < 3.5 : return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test.loc[:, 'stars'] = d_test.loc[:, 'stars'].apply(sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(label, k):\n",
    "    label = np.asfarray(label)[:k]\n",
    "    if label.size:\n",
    "        return label[0] + np.sum(label[1:] / np.log2(np.arange(2, label.size + 1)))\n",
    "\n",
    "    return 0\n",
    "\n",
    "def ndcg(dataframe, k):\n",
    "    ndcg_list = []\n",
    "    for uid in dataframe.user_id.unique():\n",
    "        label_temp = dataframe.loc[dataframe.user_id == uid]['stars'].tolist()\n",
    "\n",
    "        idcg = dcg(sorted(label_temp, reverse=True), k)\n",
    "\n",
    "        if not idcg:\n",
    "            return 0 \n",
    "\n",
    "        ndcg_list.append(dcg(label_temp, k) / idcg)\n",
    "    return np.mean(ndcg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sa_metrics(dataframe, top_k):\n",
    "    # metrics for Sentiment Analysis\n",
    "    item = dataframe.groupby(['user_id'])['stars'].sum()\n",
    "    precision_k, recall_k, f1_k, ndcg_k = [], [], [], []\n",
    "    for k in top_k:\n",
    "        precision, recall, f1_score, ndcg_score = [], [], [], []\n",
    "        for uid in tqdm(dataframe.loc[:, 'user_id'].unique(), desc=f'Tok@{k} evaluating..'):\n",
    "            new_df = dataframe.loc[dataframe.loc[:, 'user_id'] == uid].copy()\n",
    "\n",
    "            new_df = new_df.sort_values(by = ['yhat'], ascending=False).head(k)\n",
    "            pr_temp = sum(new_df.loc[:, 'stars']) / k \n",
    "            re_temp = sum(new_df.loc[:, 'stars']) / item[uid] if item[uid] != 0 else 0 \n",
    "            pr_re = pr_temp + re_temp \n",
    "            f1_temp = (2 * pr_temp * re_temp) / pr_re if pr_re != 0 else 0\n",
    "            precision.append(pr_temp)\n",
    "            recall.append(re_temp)\n",
    "            f1_score.append(f1_temp)\n",
    "            ndcg_score.append(ndcg(new_df, k))\n",
    "        \n",
    "        precision_k.append(np.mean(precision))\n",
    "        recall_k.append(np.mean(recall))\n",
    "        f1_k.append(np.mean(f1_score))\n",
    "        ndcg_k.append(np.mean(ndcg_score))\n",
    "\n",
    "    outputs = pd.DataFrame({\n",
    "        'recall': recall_k, \n",
    "        'precision': precision_k, \n",
    "        'f1_score': f1_k, \n",
    "        'ndcg': ndcg_k\n",
    "    }, index=top_k)\n",
    "    return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tok@10 evaluating..: 100%|██████████| 25307/25307 [00:19<00:00, 1267.22it/s]\n",
      "Tok@20 evaluating..: 100%|██████████| 25307/25307 [00:19<00:00, 1282.35it/s]\n",
      "Tok@40 evaluating..: 100%|██████████| 25307/25307 [00:19<00:00, 1273.14it/s]\n",
      "Tok@50 evaluating..: 100%|██████████| 25307/25307 [00:19<00:00, 1274.96it/s]\n",
      "Tok@80 evaluating..: 100%|██████████| 25307/25307 [00:19<00:00, 1267.26it/s]\n",
      "Tok@100 evaluating..: 100%|██████████| 25307/25307 [00:19<00:00, 1270.00it/s]\n"
     ]
    }
   ],
   "source": [
    "ngcf_results = sa_metrics(d_test, [10, 20, 40, 50, 80, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.889048</td>\n",
       "      <td>0.501308</td>\n",
       "      <td>0.576639</td>\n",
       "      <td>0.893569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.956685</td>\n",
       "      <td>0.302053</td>\n",
       "      <td>0.414979</td>\n",
       "      <td>0.892741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.974539</td>\n",
       "      <td>0.163428</td>\n",
       "      <td>0.257015</td>\n",
       "      <td>0.892621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.976297</td>\n",
       "      <td>0.132321</td>\n",
       "      <td>0.215552</td>\n",
       "      <td>0.892611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.977881</td>\n",
       "      <td>0.083916</td>\n",
       "      <td>0.145330</td>\n",
       "      <td>0.892603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.978195</td>\n",
       "      <td>0.067402</td>\n",
       "      <td>0.119485</td>\n",
       "      <td>0.892601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       recall  precision  f1_score      ndcg\n",
       "10   0.889048   0.501308  0.576639  0.893569\n",
       "20   0.956685   0.302053  0.414979  0.892741\n",
       "40   0.974539   0.163428  0.257015  0.892621\n",
       "50   0.976297   0.132321  0.215552  0.892611\n",
       "80   0.977881   0.083916  0.145330  0.892603\n",
       "100  0.978195   0.067402  0.119485  0.892601"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngcf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Oct 12 2021, 13:49:34) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
