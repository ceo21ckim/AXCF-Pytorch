{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm \n",
    "import pandas as pd \n",
    "\n",
    "from __init__ import * \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    seed = 42\n",
    "    max_seq_length = 256\n",
    "    batch_size = 2048\n",
    "    hidden_dim = 256\n",
    "    num_epochs = 100\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(x):\n",
    "    if x >= 3.5 : return 1\n",
    "    elif x < 3.5 : return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(YELP_DIR, 'train.csv')\n",
    "valid_path = os.path.join(YELP_DIR, 'valid.csv')\n",
    "test_path = os.path.join(YELP_DIR, 'test.csv')\n",
    "\n",
    "train = pd.read_csv(train_path, encoding='utf-8-sig')\n",
    "valid = pd.read_csv(valid_path, encoding='utf-8-sig')\n",
    "test = pd.read_csv(test_path, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self, args, dataframe, tokenizer):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.data = dataframe \n",
    "        self.reviews = dataframe.text \n",
    "        self.labels = dataframe.stars\n",
    "        self.max_seq_length = args.max_seq_length\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        review = self.reviews[idx]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            review, \n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_seq_length, \n",
    "            padding='max_length', \n",
    "            return_token_type_ids=False, \n",
    "            return_attention_mask=False,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids']\n",
    "\n",
    "        return (\n",
    "            torch.tensor(input_ids, dtype=torch.long),\n",
    "            torch.tensor(self.labels[idx], dtype = float) # labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, hidden_dim, n_classes, n_layers, bidirectional=False, drop_rate=None):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = embedding_dim ,\n",
    "            hidden_size = hidden_dim, \n",
    "            num_layers = n_layers, \n",
    "            batch_first = True, \n",
    "            bidirectional=bidirectional, \n",
    "            dropout = drop_rate\n",
    "        )\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "        self.fc = nn.Linear(2*hidden_dim if bidirectional else hidden_dim, n_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        # self._init_weight()\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        embedding = self.embedding(idx)\n",
    "        if self.drop_rate != 0:\n",
    "            self.dropout(embedding)\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(embedding)\n",
    "\n",
    "        if self.lstm.bidirectional:\n",
    "            output = torch.cat([hidden[-1], hidden[-2]], dim = -1)\n",
    "            output = self.dropout(output)\n",
    "        else:\n",
    "            output = self.dropout(output)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            \n",
    "            elif isinstance(m, nn.LSTM):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'bias' in name:\n",
    "                        nn.init.zeros_(param)\n",
    "                    \n",
    "                    elif 'weight' in name:\n",
    "                        nn.init.orthogonal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:, 'stars'] = train.loc[:, 'stars'].apply(sentiment_score)\n",
    "valid.loc[:, 'stars'] = valid.loc[:, 'stars'].apply(sentiment_score)\n",
    "test.loc[:, 'stars'] = test.loc[:, 'stars'].apply(sentiment_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args)\n",
    "\n",
    "trainset = LSTMDataset(args, train, tokenizer)\n",
    "validset = LSTMDataset(args, valid, tokenizer)\n",
    "testset = LSTMDataset(args, test, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(trainset, batch_size=args.batch_size, num_workers=4)\n",
    "valid_dataloader = DataLoader(validset, batch_size=args.batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 512\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "n_layers = 2 \n",
    "bidirectional = True \n",
    "dr_rate = 0 \n",
    "lr = 1e-3\n",
    "\n",
    "models = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dr_rate).to(args.device)\n",
    "\n",
    "optimizer = optim.Adam(models.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss().to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elapsed_time(start, end):\n",
    "    elapsed_secs = end - start \n",
    "    elapsed_mins = (end - start) // 60\n",
    "    return elapsed_mins, round(elapsed_secs, 2)\n",
    "\n",
    "def calc_accuracy(pred_y, true_y):\n",
    "    pred_y = torch.sigmoid(pred_y)\n",
    "    return ((pred_y > 0.5) == true_y).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|          | 0/425 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10607/1158650337.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10607/56533497.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 692\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "\n",
    "train_loss_list, train_acc_list = [], []\n",
    "valid_loss_list, valid_acc_list = [], []\n",
    "\n",
    "for epoch in range(1, args.num_epochs + 1):\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "    valid_loss, valid_acc = 0, 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    models.train()\n",
    "    for reviews, labels in tqdm.tqdm(train_dataloader, desc = 'training...'):\n",
    "        reviews = reviews.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        pred_y = models(reviews).squeeze()\n",
    "        loss = criterion(pred_y, labels)\n",
    "        train_acc += calc_accuracy(pred_y, labels)\n",
    "        train_loss += loss.item() / len(pred_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    models.eval()\n",
    "    with torch.no_grad():\n",
    "        for reviews, labels in tqdm.tqdm(valid_dataloader, desc = 'evaluating...'):\n",
    "            reviews = reviews.to(args.device)\n",
    "            labels = labels.float().to(args.device)\n",
    "\n",
    "            pred_y = models(reviews).squeeze()\n",
    "            loss = criterion(pred_y, labels)\n",
    "            valid_acc += calc_accuracy(pred_y, labels)\n",
    "            valid_loss += loss.item() / len(pred_y)\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_mins, elapsed_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "\n",
    "    train_acc /= len(train_dataloader)\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    valid_acc /= len(valid_dataloader)\n",
    "    valid_loss /= len(valid_dataloader)\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    valid_acc_list.append(valid_acc)\n",
    "    print(f'epoch [{epoch}/{args.num_epochs}] | elapsed time: {elapsed_mins}m, {elapsed_secs:.2f}s')\n",
    "    print(f'train loss: {train_loss:.6f}\\ttrain accuracy: {train_acc*100:.2f}%')\n",
    "    print(f'test loss: {valid_loss:.6f}\\ttest accuracy: {valid_acc*100:.2f}% \\n')\n",
    "\n",
    "    if best_loss > valid_loss :\n",
    "        best_loss = valid_loss\n",
    "        lstm_path = os.path.join(BASE_DIR, 'baseline_parameters')\n",
    "        if not os.path.exists(lstm_path):\n",
    "            os.makedirs(lstm_path)\n",
    "        torch.save(models.state_dict(), os.path.join(lstm_path, 'lstm_parameters_2.pt'))\n",
    "\n",
    "results = pd.DataFrame([train_loss_list, valid_loss_list, train_acc_list, valid_acc_list], index = ['train_loss', 'test_loss', 'train_acc', 'test_acc']).T \n",
    "\n",
    "\n",
    "save_path = os.path.join(BASE_DIR, 'baseline')\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "results.to_csv(os.path.join(save_path, 'lstm_results_2.csv'), encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
